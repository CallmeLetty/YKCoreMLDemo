#import os
#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
#
#from transformers import pipeline
#
## ç°åœ¨åº”è¯¥å¯ä»¥ä¸‹è½½äº†
#classifier = pipeline("sentiment-analysis",
#                     model="uer/roberta-base-finetuned-dianping-chinese")
#
#result = classifier("èƒŒæ™¯éŸ³ä¹å¤ªå¤§å£°ï¼Œç›–è¿‡äººå£°ï¼Œå¬å¾—å¤´ç–¼ã€‚")
#print(result)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®
train_data = [
    ("è¿™ä¸ªç”µå½±å¤ªå¥½çœ‹äº†", 1),
    ("éå¸¸æ£’çš„ä½“éªŒ", 1),
    ("æˆ‘å¾ˆå–œæ¬¢è¿™ä¸ªäº§å“", 1),
    ("è´¨é‡å¾ˆå¥½å€¼å¾—è´­ä¹°", 1),
    ("æœåŠ¡æ€åº¦è¶…çº§å¥½", 1),
    ("ç‰©è¶…æ‰€å€¼å¼ºçƒˆæ¨è", 1),
    ("å‘³é“å¾ˆä¸é”™", 1),
    ("ç¯å¢ƒä¼˜é›…èˆ’é€‚", 1),
    ("æ€§ä»·æ¯”å¾ˆé«˜", 1),
    ("ä¼šå†æ¥çš„", 1),
    ("å¤ªå·®äº†å®Œå…¨ä¸æ¨è", 0),
    ("æµªè´¹æ—¶é—´å’Œé‡‘é’±", 0),
    ("éå¸¸å¤±æœ›", 0),
    ("è´¨é‡å¤ªå·®äº†", 0),
    ("æœåŠ¡æ€åº¦æ¶åŠ£", 0),
    ("ä¸å€¼è¿™ä¸ªä»·æ ¼", 0),
    ("éš¾åƒæ­»äº†", 0),
    ("ç¯å¢ƒå¾ˆå·®", 0),
    ("æ€§ä»·æ¯”ä½", 0),
    ("ä¸ä¼šå†æ¥äº†", 0),
]

# è¯æ±‡è¡¨
class Vocabulary:
    def __init__(self):
        self.word2idx = {"<PAD>": 0, "<UNK>": 1}
        self.idx2word = {0: "<PAD>", 1: "<UNK>"}
        self.idx = 2
    
    def add_word(self, word):
        if word not in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            self.idx += 1

# åˆ†è¯
def tokenize(text):
    return list(text)

# æ„å»ºè¯æ±‡è¡¨
vocab = Vocabulary()
for text, _ in train_data:
    for char in tokenize(text):
        vocab.add_word(char)

# æ•°æ®é›†
class TextDataset(Dataset):
    def __init__(self, data, vocab, max_len=30):
        self.data = data
        self.vocab = vocab
        self.max_len = max_len
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        text, label = self.data[idx]
        tokens = tokenize(text)
        indices = [self.vocab.word2idx.get(token, 1) for token in tokens]
        
        if len(indices) < self.max_len:
            indices += [0] * (self.max_len - len(indices))
        else:
            indices = indices[:self.max_len]
        
        return torch.tensor(indices), torch.tensor(label)

# æ”¹è¿›çš„æ¨¡å‹ï¼ˆåŠ å…¥åŒå‘LSTMå’ŒDropoutï¼‰
class SentimentModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):
        super(SentimentModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim,
                           batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim * 2, 2)
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # æ‹¼æ¥å‰å‘å’Œåå‘çš„æœ€åéšè—çŠ¶æ€
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        hidden = self.dropout(hidden)
        output = self.fc(hidden)
        return output

# è®­ç»ƒ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dataset = TextDataset(train_data, vocab)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

model = SentimentModel(len(vocab)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("å¼€å§‹è®­ç»ƒ...")
for epoch in range(100):
    model.train()
    total_loss = 0
    
    for texts, labels in dataloader:
        texts, labels = texts.to(device), labels.to(device)
        outputs = model(texts)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {total_loss/len(dataloader):.4f}')

# é¢„æµ‹å‡½æ•°
def predict(text, model, vocab, device, max_len=30):
    model.eval()
    tokens = tokenize(text)
    indices = [vocab.word2idx.get(token, 1) for token in tokens]
    
    if len(indices) < max_len:
        indices += [0] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    
    tensor = torch.tensor([indices]).to(device)
    
    with torch.no_grad():
        output = model(tensor)
        probabilities = torch.softmax(output, dim=1)
        prediction = torch.argmax(output, dim=1)
        confidence = probabilities[0][prediction].item()
    
    sentiment = "æ­£é¢ğŸ˜Š" if prediction.item() == 1 else "è´Ÿé¢ğŸ˜"
    return sentiment, confidence

# æµ‹è¯•
test_texts = [
    "è¿™ä¸ªçœŸçš„å¾ˆæ£’",
    "å¤ªå¤±æœ›äº†",
    "éå¸¸å¥½ç”¨æ¨è",
    "å¾ˆå·®åŠ²ä¸è¦ä¹°",
    "ä¸€èˆ¬èˆ¬å§"
]

print("\n" + "="*50)
print("é¢„æµ‹ç»“æœ:")
print("="*50)
for text in test_texts:
    sentiment, confidence = predict(text, model, vocab, device)
    print(f"æ–‡æœ¬: '{text}'")
    print(f"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.2%})\n")
