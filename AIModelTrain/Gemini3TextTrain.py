import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from collections import Counter
import jieba  # å¯¼å…¥ä¸­æ–‡åˆ†è¯å·¥å…·
import time

# ---------------------------------------------------------
# 1. å‡†å¤‡æ•°æ®ï¼ˆæ¨¡æ‹Ÿä¸€äº›ä¸­æ–‡è¯­æ–™ï¼‰
# ---------------------------------------------------------
# å®é™…åº”ç”¨ä¸­ï¼Œä½ ä¼šä» CSV æˆ– TXT æ–‡ä»¶ä¸­è¯»å–è¿™äº›æ•°æ®
raw_train_data = [
    (1, "è¿™ä¸ªæ’­å®¢å†…å®¹å¾ˆæ£’ï¼Œå­¦åˆ°äº†å¾ˆå¤š"),
    (1, "éå¸¸å–œæ¬¢è¿™ä¸ªåšä¸»ï¼Œå£°éŸ³å¥½å¬"),
    (1, "è´¨é‡å¾ˆå¥½ï¼Œç‰©è¶…æ‰€å€¼ï¼Œæ¨èè´­ä¹°"),
    (1, "æŒºä¸é”™çš„ï¼Œä¸‹æ¬¡è¿˜ä¼šå†æ¥"),
    (1, "è€å¸ˆè®²å¾—éå¸¸æ¸…æ™°ï¼Œå—ç›ŠåŒªæµ…"),
    (0, "è¿™ä¸œè¥¿è´¨é‡å¤ªå·®äº†ï¼Œåƒä¸‡åˆ«ä¹°"),
    (0, "æ’­å®¢å†…å®¹å¾ˆæ— èŠï¼Œå¬ä¸ä¸‹å»"),
    (0, "æœåŠ¡æ€åº¦æå…¶æ¶åŠ£ï¼Œå†ä¹Ÿä¸æ¥äº†"),
    (0, "å®Œå…¨æ˜¯æµªè´¹æ—¶é—´ï¼Œå·®è¯„"),
    (0, "è§†é¢‘ç”»è´¨å¾ˆç³Šï¼Œçœ‹å¾—çœ¼ç›ç–¼"),
] * 50 # æŠŠæ•°æ®å¤åˆ¶50ä»½ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå°è§„æ¨¡è®­ç»ƒé›†

# ---------------------------------------------------------
# 2. ä¸­æ–‡åˆ†è¯ä¸è¯è¡¨æ„å»º
# ---------------------------------------------------------
def chinese_tokenizer(text):
    # ä½¿ç”¨ jieba è¿›è¡Œç²¾ç¡®æ¨¡å¼åˆ†è¯
    return list(jieba.cut(text))

class ChineseVocab:
    def __init__(self, data, min_freq=1):
        counter = Counter()
        for _, text in data:
            counter.update(chinese_tokenizer(text))
        
        # <unk>ä»£è¡¨æœªçŸ¥è¯ï¼Œ<pad>ç”¨äºå¡«å……é•¿åº¦
        self.stoi = {'<unk>': 0, '<pad>': 1}
        self.itos = {0: '<unk>', 1: '<pad>'}
        
        idx = 2
        for word, freq in counter.items():
            if freq >= min_freq:
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1
                
    def encode(self, text):
        return [self.stoi.get(word, 0) for word in chinese_tokenizer(text)]
    
    def __len__(self):
        return len(self.stoi)

# åˆå§‹åŒ–è¯è¡¨
vocab = ChineseVocab(raw_train_data)
print(f"è¯è¡¨æ„å»ºå®Œæˆï¼Œå…±æœ‰ {len(vocab)} ä¸ªè¯ã€‚")
print(f"ç¤ºä¾‹åˆ†è¯: {chinese_tokenizer('è¿™æ®µæ’­å®¢è¯´çš„ä¸é”™')}")

# ---------------------------------------------------------
# 3. æ•°æ®åŠ è½½å™¨ (Dataset & DataLoader)
# ---------------------------------------------------------
class SimpleDataset(Dataset):
    def __init__(self, data, vocab):
        self.data = data
        self.vocab = vocab
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        label, text = self.data[idx]
        return label, self.vocab.encode(text)

def collate_fn(batch):
    labels, texts, offsets = [], [], [0]
    for _label, _text_ids in batch:
        labels.append(_label)
        t = torch.tensor(_text_ids, dtype=torch.int64)
        texts.append(t)
        offsets.append(t.size(0))
    
    labels = torch.tensor(labels, dtype=torch.float32)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    texts = torch.cat(texts)
    return labels, texts, offsets

train_ds = SimpleDataset(raw_train_data, vocab)
train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)

# ---------------------------------------------------------
# 4. æ­å»ºæ¨¡å‹ (å’Œä¹‹å‰ä¸€æ ·)
# ---------------------------------------------------------
class ChineseClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=64):
        super().__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        self.fc = nn.Linear(embed_dim, 1)
        
    def forward(self, text, offsets):
        embedded = self.embedding(text, offsets)
        return self.fc(embedded)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ChineseClassifier(len(vocab)).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # æ¢ä¸ªæ›´å¥½çš„ä¼˜åŒ–å™¨ Adam

# ---------------------------------------------------------
# 5. è®­ç»ƒ
# ---------------------------------------------------------
print("å¼€å§‹è®­ç»ƒä¸­æ–‡æ¨¡å‹...")
model.train()
for epoch in range(20): # ç»ƒ20è½®
    total_loss = 0
    for label, text, offsets in train_loader:
        label, text, offsets = label.to(device), text.to(device), offsets.to(device)
        
        optimizer.zero_grad()
        output = model(text, offsets).squeeze()
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

# ---------------------------------------------------------
# 6. æµ‹è¯•
# ---------------------------------------------------------
def predict_chinese(text):
    model.eval()
    with torch.no_grad():
        ids = torch.tensor(vocab.encode(text), dtype=torch.int64).to(device)
        offset = torch.tensor([0]).to(device)
        output = model(ids, offset)
        prob = torch.sigmoid(output).item()
        return "æ­£é¢ ğŸ˜„" if prob > 0.5 else "è´Ÿé¢ ğŸ˜¡", prob

print("\n--- æµ‹è¯•å¼€å§‹ ---")
texts = ["è¿™æ®µæ’­å®¢è¯´çš„ä¸é”™", "è¿™ä¸ªä¸œè¥¿çœŸå¿ƒä¸å¥½ç”¨", "åšä¸»è®²å¾—å¾ˆåˆ°ä½"]
for t in texts:
    res, prob = predict_chinese(t)
    print(f"å¥å­: {t} -> é¢„æµ‹: {res} (æ¦‚ç‡: {prob:.4f})")
